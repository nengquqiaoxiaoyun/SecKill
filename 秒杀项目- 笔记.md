

# 第四章-压测

## JMeter压测

### 吞吐量（Throughput）

吞吐量指的是单位时间内系统处理请求的数量 
一般来说越高越好，但它并不是唯一的衡量系统性能的指标。
当系统中出现瓶颈时，增加吞吐量可能会导致其他指标，如响应时间或数据错误率，变差。
因此，在考虑吞吐量时，还需要考虑其他指标，以确保系统的性能和可靠性。

对于无并发的应用系统而言，吞吐量与响应时间成严格的反比关系，实际上此时吞吐量就是响应时间的倒数。（响应时间越短吞吐量越高，反之越低）
对于单用户的系统，响应时间（或者系统响应时间和应用延迟时间）可以很好地度量系统的性能，
但对于并发系统，通常需要用吞吐量作为性能指标。 

**高性能系统**：能承受高并发，返回快速
**发现容量问题**：并发是否高（系统可承受的并发）

**server端并发线程数上不去导致tps上不去导致客户端被拒绝链接发生各种问题**
**为什么tomcat线程数上不去？**

```yaml
spring-configuration-metadata.json配置（版本不同配置名会有区别）
server.tomcat.accept-count：等待队列长度，默认100
server.tomcat.max-connections：最大可被连接数，默认8192
server.tomcat.threads.max：最大工作线程数，默认200
4核8G推荐值800
server.tomcat.threads.min-spare：最小工作线程数，默认10
默认配置下，链接超过8192后出现拒绝连接情况，
触发的请求超过200+100后拒绝处理
```

### 一些命令

```
ps -ef | grep java
查看所有java进程
pstree -p <进程id>
查看进程中所有的线程
pstree -p <进程id> | wc -l
查看该进程中所有的线程数量
top -H 
查看机器性能
需要关注 %Cpu（s）
xx us用户态下的cpu耗时，xx sy 内核空间对cpu的占有率（系统调用的）这两个参数加起来不可以超过100%
load average: xx, xx, xx 表示最近一分钟、五分钟、十五分钟cpu的load数量，越低越好，对于2核的cpu的load average应该控制在2以内
超过2表示cpu非常忙
```

总结：

**使用压测工具进行压力测试，发现服务并发容量问题**
**需要掌握几个命令查看线程数量，以及基本的性能查看**
**特别注意tomcat的几个配置（等待线程、最大最小线程等）**

## nginx

学习目标：

nginx反向代理负载均衡
分布式会话管理
redis实现分布式会话存储

主要业务功能：

1. 使用nginx作为web服务器（提供静态页面资源访问）
2. 使用nginx作为动静分离服务器
3. 使用nginx作为反向代理服务器 



当前问题：gethosts中全局路径只有一个服务，nginx怎么提供两个服务的访问+ 已解决

添加gethost.js，作为全局接口地址使用（**gethost的地址应该为nginx服务主机的地址（miaoshaserver）**，这边写的地址经过nginx代理会直接反映到抓包地址上，比如这里写的localhost则抓包地址就是localhost，如果写的miaoshaserver就抓到miaoshaserver）

使用OpenResty作为Nginx的开发框架

[安装OpenResty](https://openresty.org/cn/installation.html)

```shell
提前安装开发库
yum install pcre-devel openssl-devel gcc curl
提前下载openresty的版本，进入目录后：
tar -zxvf openresty-VERSION.tar.gz
cd openresty-VERSION/
./configure
make
sudo make install
```

安装完成后OpenResty默认在` /usr/local/openresty`目录下

### nginx web服务器（nginx默认80端口）

启动：在nginx目录下使用命令 **`sbin/nginx -c conf/nginx.conf`**
在修改nginx配置后使用 **`sbin/nginx -s reload`** 可以无缝重启（用户链接不会断，进程会变  ）

sbin/nginx -s quit`等待请求结束 后关闭nginx

`sbin/nginx -s stop`直接关闭

### 前端资源部署

j将前端资源放入nginx目录下的html目录中即可

**秒杀项目静态请求访问 resourses目录，其他请求当作动态请求做方向代理请求**

**为了达到效果，静态资源应该放在resources下**

nginx配置修改如下

```nginx
location /resources/ {
    # 原来为 root html;
    alias /usr/local/openresty/nginx/html/resources/;
    index index.html index.htm;
}

alias 当location命中了/resources/目录后替换为alias后面的目录
```

nginx的location节点path特定resources：静态资源路径

location节点其他路径：动态资源用

### 反向代理服务器

nginx反向代理需要修改配置

- 设置upstream server
- 设置动态请求location为proxy pass路径

修改nginx.conf添加如下配置

```nginx
http {
 
    ...
    
    upstream backend_server {
         # weight代表权重，这边表示这两个服务的轮询为1比1
         server 192.168.127.131:8089 weight=1;
         server 192.168.127.132:8089 weight=1;
	}

server {
    
    ...
    
    location / {
        proxy_pass http://backend_server;
        proxy_set_header Host $http_host:$proxy_port;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
    
	}
}
```

配置修改完成后重新加载nginx

**修改配置完成后已经完成了服务的反向代理，访问miaoshaserver/接口地址，帮我们代理到了真正的服务上**

- 开启tomcat access log验证（可以查看tomcat被请求的日志）

access log采用异步记录日志，会消耗一部分性能，建议生产中打开

修改application.yaml添加如下配置

```yaml
server:
    accesslog:
      enabled: true
      directory: /opt/server/SecKill/tomasslog
      # h: host(ip) l: u: remote user t: 时间 %r: 请求方法 请求url s: http状态码 b: 字节大小 D: 处理请求时长 
 :     pattern: '%h %l %u %t "%r" %s %b %D'
```

#### 长链接

使用长链接保证网络建联消耗的减少（对于数据库，使用druid可以为我们减少链接）

修改nginx.conf配置

```nginx
http {
 
    ...
    
    upstream backend_server {
         # weight代表权重，这边表示这两个服务的轮询为1比1
         server 192.168.127.131:8089 weight=1;
         server 192.168.127.132:8089 weight=1;
        # 长链接时长
        keepalive 30;
	}

server {
    
    ...
    
    location / {
        proxy_pass http://backend_server;
        proxy_set_header Host $http_host:$proxy_port;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
    
	}
}
```

使用命令

`netstat -an | grep <ipaddr> | grep ESTABLISH`

可以查看建立的链接，多次执行该命令可以发现使用了长连接，未使用长链接查询出来的链接每次都不同

### 高性能原因

#### epoll多路复用

从Java BIO模型说起，当客户端需要向服务端发送数据时执行的socket.write操作需要等待网络中信息传输完成。在这样阻塞式的模型下难以完成需求。Linux Select模型，假设服务端监听100个客户端的连接，是否有变化，没有变化继续睡眠，若有变化唤醒自己找到发生变化的一个或者多个执行read操作。它是一个遍历循环的操作。因为是一个遍历查询效率比较低，有多少查多少。事实上只能监听1024个。epoll模型也是监听连接，在监听的时候设置回调函数，若有变化就会进行回调。不需要执行循环而且没有数量限制。

Java NIO模型就是借用了select和epoll模型的思想。在Linux2.6内核以上会议epoll模型运作，2.6以下以select模型运作。  

依靠epoll解决IO阻塞回调问题

#### master- worker进程模型   

![image-20230516151708622](秒杀项目- 笔记.assets\image-20230516151708622.png)

依靠master-worker进程模型完成平滑的过度和重启，并且基于worker的单线程模型结合epoll多路复用机制完成高效的操作

#### 协程机制

基于协程机制将每个用户的请求对应到线程中的某一个协程中，在协程中使用epoll多路复用机制完成同步调用开发，完成高性能操作

### 分布式会话实现

使用redis session代替基于cookie传输sessionid

问题：存储在了redis后值是唯一的，也就是用其他账号登陆会覆盖之前账号，需要测试

#### 基于Redis的Tomcat session

导入依赖

```xml
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-redis</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.session</groupId>
            <artifactId>spring-session-data-redis</artifactId>
            <version>2.0.5.RELEASE</version>
        </dependency>
```

添加配置类

```java
package com.huakai.config;


import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession;
import org.springframework.stereotype.Component;

/**
 * @author: huakaimay
 * @since: 2023-05-17
 */
@Component
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 3600)
public class RedisConfig {
}

```

配置文件添加redis配置

```yaml
spring:
  redis:
    host: 192.168.127.131
    database: 1
    port: 6379
    jedis:
      pool:
        max-active: 50
        min-idle: 20
```

原来项目中使用session保存的信息不需要额外修改代码，会被redis存储

#### 基于token传输类似sessionid

使用session有一定的局限性，而token可以满足各种应用场所（手机、设备等）

原本session存储的isLogin和用户信息改为redis存储token和用户信息并下发给前端，前端请求需要带上token，根据前端token和后端redis存储进行匹配

# 第五章-缓存

## 本章目标

掌握多级缓存的定义

掌握redis缓存，本地缓存

掌握热点nginx lua缓存

## 缓存设计

- 用快速存取设备，用内存
- 将缓存推到离用户最近的地方
- 脏缓存数据

缓存设计需要考虑的是快速读取可以用内存，缓存作用在哪个节点上面，应该设计到离用户最近的地方。引入缓存需要考虑缓存同步的问题，数据库数据变更，缓存成为了脏数据。关键数据应该存储在数据库中。

## redis

[redis面试](https://xiaolincoding.com/redis/)

### sentinal哨兵模式

### 集群cluster模式

### 商品详情动态内容实现 

```java
   @GetMapping("/get")
    public CommonReturnType get(@RequestParam("id") Integer id) throws BussinesssError {

        ItemDto itemDto = redisService.get("item_" + id, ItemDto.class);

        if(ObjectUtils.isEmpty(itemDto)) {
            itemDto =itemService.getItemDetailById(id);
            if(!ObjectUtils.isEmpty(itemDto)) {
                redisService.put("item_" + id, new Gson().toJson(itemDto), 10, TimeUnit.MINUTES);
            } else {
                throw new BussinesssError(ErrorEnum.PARAMTER_VALIDATION_ERROR, "数据异常");
            }
        }


        PromoDto promoDto = itemDto.getPromoDto();
        if (ObjectUtils.isEmpty(promoDto)) {
            PromoDto innerPromo = new PromoDto();
            innerPromo.setStatus(0);
            itemDto.setPromoDto(innerPromo);
        }

        return CommonReturnType.create(itemDto);

    }

```

### 压测

1使用redis压测结果 （2000线程数循环100次60s启动）

![image-20230523102544437](秒杀项目- 笔记.assets\image-20230523102544437.png)

## 本地热点缓存

[Java本地缓存技术选型（Guava Cache、Caffeine、Ehcache）](https://juejin.cn/post/6844904199453409294)

本地热点缓存是一种将数据存储在本地内存中的缓存方式，以提高系统性能和响应速度。当应用程序访问某些数据时，**如果该数据经常被访问，那么就可以将其存储在本地缓存中**，以便下次直接从本地缓存获取数据，避免频繁地访问数据库或其他远程服务。

**本地热点缓存通常用于存储一些访问频率比较高的数据，例如配置信息、静态数据等**。对于这些数据，虽然它们**不会经常发生变化**，但是由于它们被频繁地访问，因此每次都从数据库或其他远程服务获取数据会影响系统的响应速度和性能。

本地热点缓存的优点包括：

1. 提高系统响应速度：将经常访问的数据存储在本地缓存中，可以避免频繁地访问数据库或其他远程服务，从而提高系统的响应速度。
2. **减少网络请求次数**：由于数据存储在本地缓存中，可以减少与远程服务的交互，从而减少网络请求次数，降低网络带宽的消耗。**相对于redis而言，本地缓存可以减少网络开销。**
3. 改善系统性能：使用本地热点缓存可以减少与数据库或其他远程服务的交互，从而降低系统的负载和消耗，提升系统性能。

本地热点缓存的缺点包括：

1. 数据一致性问题：由于数据存储在本地缓存中，可能会出现缓存和数据库中数据不一致的情况，因此需要实现有效的缓存更新机制，以保证数据的一致性。
2. 内存占用问题：本地热点缓存需要将数据存储在内存中，如果缓存的数据量过大，可能会导致系统内存使用率过高，从而影响系统的稳定性。因此需要合理设置缓存的大小和清除策略，以避免内存溢出等问题。

### 压测

多级缓存--本地缓存+redis缓存压测结果（2000线程数循环100次60s启动）

![image-20230523103118063](秒杀项目- 笔记.assets\image-20230523103118063.png)

多级缓存--本地缓存+redis缓存压测结果（1000线程数循环100次30s启动）

![image-20230523103326279](秒杀项目- 笔记.assets\image-20230523103326279.png)

 

## nginx proxy cache缓存 

nginx proxy cache 直接从磁盘读取文件，导致效率无法和内存相比较

```nginx
#gzip on;
...


# 申明一个cache缓存节点的内容
proxy_cache_path /usr/local/openresty/nginx/tmp_cache levels=1:2 keys_zone=tmp_cache:100m inactive=7d max_size=10g;

server {
    ...
        
        location / {
        proxy_cache tmp_cache;
        proxy_cache_key $uri;
        proxy_cache_valid 200 206 304 302 7d;
        }
    
}
```

## nginx lua 实战

访问该路径时会在lua脚本下输出

```nginx
server {
    location /staticitem/get {
        default_type text/html;
        content_by_lua_file ../lua/staticitem.lua;
    }
}
```

lua目录下（自己建的目录，和nginx目录同级）staticitem.lua

```lua
ngx.say("hello static item lua");
```

## OpenResty实战

### Hello World

lua目录下新建helloworld.lua

```lua
 nginx.exec("/item/get?id=6");
```

修改nginx.conf

```nginx
server {
    location /helloworld {
    content_bu_lua_file ../lua/helloworld.lua;
	}
}
```

### Shared dic

修改nginx.conf

```nginx
upstream ...

lua_shared_dict my_cache 128m;

server {
    location /luaitem/get{
        default_type "application/json";
        content_by_lua_file ../lua/itemsharedic.lua;
    }
}
```

在lua目录下新建itemsharedic.lua

```lua
function get_from_cache(key)
    local cache_ngx = ngx.shared.my_cache
    local value = cache_ngx:get(key)
    return value
end

function set_to_cache(key,value,exptime)
    if not exptime then 
        	exptime = 0
    end
    local cache_ngx = ngx.shared.my_cache
    local succ,err,forcible = cache_ngx:set(key,value,exptime)
    return succ
end

-- 获取id参数
local args = ngx.req.get_uri_args()
local id = args["id"]
local item_model = get_from_cache("item_"..id)
-- 如果缓存没有则去获取原来的接口并存入缓存
if item_model == nil then
    local resp = ngx.location.capture("/item/get?id="..id)
    item_model = resp.body
    set_to_cache("item_"..id,item_model,1*60)
end
ngx.say(item_model)
```

#### 压测

1000线程数循环100次30s启动

![image-20230529153011632](秒杀项目- 笔记.assets\image-20230529153011632.png)

负载压力都转移到了nginx

### Redis支持

lua目录下创建itemredis.lua

```lua
local args = ngx.req.get_uri_args()
local id = args["id"]
local redis = require "resty.redis"
local cache = redis:new()
local ok,err = cache:connect("192.168.127.131", 6379)
local item_model = cache:get("item_"..id)
if item_model == ngx.null or item_model == nil then
    -- 这边正常需要存入redis，这里我们的nginx redis只做读取，redis存入操作已有后端完成
    local resp = ngx.location.capture("/item/get?id="..id)
    item_model = resp.body
end
ngx.say(item_model)
```

修改nginx.conf

```nginx
server {
       location /luaitem/get{
        default_type "application/json";
        content_by_lua_file ../lua/itemredis.lua;
    } 
}
```

使用该方式本地压测效果并不理想

## 部署环境架构

![image-20230530092942649](秒杀项目- 笔记.assets\image-20230530092942649.png)

redis slave可以部署多个分摊压力

# 第七-八章 缓存-事务性消息

## 本章目标

掌握高效交易验证方式

掌握缓存库存模型

掌握异步化事物型消息模型

掌握库存售罄模型

## RocketMQ

[RocketMQ 快速开始](https://rocketmq.apache.org/zh/docs/4.x/introduction/02quickstart)

### 安装

```shell
wget https://archive.apache.org/dist/rocketmq/4.4.0/rocketmq-all-4.4.0-bin-release.zip
unzip rocketmq-all-4.4.0-bin-release.zip

# 添加环境变量
vi ~/.bashrc
export ROCKETMQ_HOME=/opt/module/rocketmq-all-4.4.0-bin-release
export PATH=$PATH:$ROCKETMQ_HOME/bin
source ~/.bashrc
```

进入安装目录下

```shell
# 启动nameserver
nohup sh bin/mqnamesrv &
tail -f ~/logs/rocketmqlogs/namesrv.log
# 启动broker
nohup sh bin/mqbroker -n localhost:9876 &
tail -f ~/logs/rocketmqlogs/broker.log 
# 查看运行状态
sh mqadmin clusterList -n localhost:9876
```

关闭服务器

```sh
sh bin/mqshutdown broker
sh bin/mqshutdown namesrv
```

消息收发

```shell
$ export NAMESRV_ADDR=localhost:9876
$ sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer
 SendResult [sendStatus=SEND_OK, msgId= ...

$ sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer
 ConsumeMessageThread_%d Receive New Messages: [MessageExt...
```

创建topic

```shell
sh mqadmin updateTopic -n localhost:9876 -t stock -c DefaultCluster

# 查看所有topic
sh mqadmin topicList -n localhost:9876
```

## 交易性能瓶颈

交易验证完全依赖数据库

减库存操作存在库存行锁

后置处理逻辑

![image-20230531155604360](秒杀项目- 笔记.assets\image-20230531155604360.png)

**用户风控策略优化：策略缓存模型化**

**活动校验策略优化：引入活动发布流程，模型缓存化，紧急下线能力**

### 用户和item缓存模型

查询用户和item时添加缓存
```java
 /**
     * 缓存用户信息
     */
    private UserDO userInCache(Integer uesrId) {

        String cacheKey = "user_valited_" + uesrId;
        UserDO userDO = redisService.get(cacheKey, UserDO.class);

        if(ObjectUtils.isEmpty(userDO)) {
            userDO = userDOMapper.selectByPrimaryKey(uesrId);
            redisService.put(cacheKey, new Gson().toJson(userDO), 10, TimeUnit.MINUTES);
        }

        return userDO;
    }
    /**
     * 缓存商品信息
     */
    private ItemDto itemInCache(Integer id) {

        String cacheKey = "item_valited_" + id;
        ItemDto userDO = redisService.get(cacheKey, ItemDto.class);

        if(ObjectUtils.isEmpty(userDO)) {
            userDO = itemService.getItemDetailById(id);
            redisService.put(cacheKey, new Gson().toJson(userDO), 10, TimeUnit.MINUTES);
        }

        return userDO;
    }
```

### 库存行锁优化方案

- 扣减库存缓存化

1. 活动发布时同步库存进缓存（创建活动 ）创建订单时生成item的活动缓存

发布活动应该交给运营人员，注意发布时的查询数据库缓存应该分为上下架，不然可能导致库存已经扣减，缓存到了不一致的数据

2. 下单交易减缓存库存

```java
 @Override
    public void publishPromo(Integer id) throws BussinesssError {
        PromoDo promoDo = promoDoMapper.selectByPrimaryKey(id);

        if(ObjectUtils.isEmpty(promoDo))
            throw new BussinesssError(ErrorEnum.PROMO_NOT_EXIST);

        if(ObjectUtils.isEmpty(promoDo.getItemId()) || promoDo.getItemId() == 0) {
            throw new BussinesssError(ErrorEnum.PROMO_NO_ITEM);
        }

        ItemDto item = itemService.getItemDetailById(promoDo.getItemId());
        String key = "promo_item_stock_" + item.getId();
        redisService.put(key, String.valueOf(item.getStock()));

    }

    @Override
    @Transactional
    public boolean decreaseStock(Integer itemId, Integer amount) {
     //   int record = itemStockDOMapper.decreaseStock(itemId, amount);

        String key = "promo_item_stock_" + itemId;
        // result表示计算后的最新值
        Long result = redisService.decrement(key, amount);

        return result >= 0;
    }
```

该方案会导致数据库和缓存库存数据不一致

- 异步同步数据库

1. 活动发布同步库存进缓存
2. 下单交易减缓存库存
3. 异步消息扣减数据库内库存

会存在问题：

异步消息发送失败

扣减操作执行失败

下单失败无法正确回补库存 

---

异步消息应该等到事物提交后发送（Spring transtional需要等到整个方法只需结束才提交，也就是说如果在最后发送了异步消息，但是各种原因导致commit失败则异步消息已经发出导致数据不一致）

解决方案：使用事务型消息发送，若因为创建订单时的异常导致无法回滚需要引入订单流水

- 库存数据库最终一致性保证

### 操作流水

数据类型： 

- 主业务数据：master data，与业务相关的数据比如订单数量
- 操作性数据：log data，用于监控情况及时发现问题

主业务数据更多的是反映公司的核心业务，而操作型数据则更多的是反映公司的日常运作

# 第九章 流量削峰

## 本章目标

- 掌握秒杀令牌的原理和使用方式
- 掌握秒杀大闸的原理和使用方式
- 掌握队列泄洪的原理和使用方式

## 项目缺陷

- 秒杀下单接口会被脚本不停的刷
- 秒杀验证逻辑和秒杀下单接口强关联，代码冗余度高
- 秒杀验证逻辑复杂，对交易系统产生无关联负载

## 秒杀令牌

秒杀接口需要依靠令牌才能进入

秒杀的令牌由秒杀活动模块负责生成

秒杀活动模块对秒杀令牌生成全权处理，逻辑收口

秒杀下单前需要先获得秒杀令牌

### 缺陷

秒杀令牌只要活动一开始就会无限制生成（有多少用户参与秒杀就有多少令牌），影响系统性能

### 秒杀大闸

- 通过库存量初始化令牌发放量控制令牌数据（控制大闸流量）

这里的意思就是设置一个总量比如库存*3倍，只要3倍量没有用完就可以颁发令牌

- 提前校验用户和库存（用户风控前置、库存售罄前置）

缺陷：若库存量非常大仍然会有大量流量瞬间进入，对于多库存多商品等令牌限制能力弱 

## 队列泄洪

```java
private ExecutorService executorService;


@PostConstruct
private void init() {
    // 20个线程池，多出的排队
    executorService = Executors.newFixedThreadPool(20);
}

// 拥塞窗口为20的等待队列，用来队列化泄洪
Future<Object> future = executorService.submit(() -> {
    // 处理库存请求
    handleStockRequest(itemId, promoId, amount, userDO.getId());
    return null;
});
 
try {
    future.get();
} catch (InterruptedException e) {
    throw new BussinesssError(ErrorEnum.UNKNOWN_ERROR);
} catch (ExecutionException e) {
    throw new BussinesssError(ErrorEnum.UNKNOWN_ERROR);
}
```

总结：所做的一切都是为了流量削峰尤其是队列泄洪

# 第十章 防刷限流

## 本章目标

- 掌握验证码生成与验证技术
- 掌握限流原理与实现
- 掌握防黄牛技术

## 验证码

包装秒杀令牌前置，需要验证码来错峰（让用户流量错峰）

后端生成验证码存入redis，前端页面输入验证

## 限流目的

### 限流方案

- 限制并发

某一时间只允许多少个请求 

- 令牌桶算法

限制某一时刻的最大值，应对突发流量，但不能超过限定值

- 漏桶算法   

平滑网络流量，固定流量操作

### 限流代码实现

在Guava中，可以使用**RateLimiter**类来实现限流

```java
public class Example {
    public static void main(String[] args) {
        // 每秒钟只允许执行2次
        RateLimiter rateLimiter = RateLimiter.create(2.0);

        while (true) {
            // 获取令牌
            rateLimiter.acquire();

            // 执行业务逻辑
            System.out.println("execute...");
        }
    }
}
```

创建一个每秒钟允许执行2次操作的RateLimiter对象。在循环中，通过调用`acquire`方法来获取令牌，并执行我们的业务逻辑。如果此时没有可用的令牌，`acquire`方法将会阻塞，直到有足够的令牌为止。   

需要注意的是，RateLimiter类不保证精确的速率控制，而是尽力控制速率。因此，在使用RateLimiter进行限流时，应该根据实际情况进行调整和测试。

## 防刷

排队、限流、令牌均只能控制总流量，无法控制黄牛流量

传统防刷： 

限制一个会话同一秒/分钟接口调用多少次：多会话接入绕开无效

限制ip同一秒/分钟接口调用多少次：数量不好控制，容易误伤

# 项目总结

![image-20230615151447730](秒杀项目- 笔记.assets\image-20230615151447730.png)

 



我在21年8月开始学习这个项目的基础课程，虽然只完成了一些简单的接口，但是并没有继续深入学习。时隔两年，我成功地完成了进阶课程，主要因为看到了目录中高级名词的吸引力，例如流量错峰、秒杀令牌和限流等。总体而言，这门课程非常干，特别是对于缺乏相关只是的人来说，尤其是前面几张的压测和性能优化而后续章节相对逊色。在学习完整门课程之后，我发现原本害怕的那些高级名词其实并不难理解和实现。作者为了演示目的，可能会在功能方面出现一些bug（整体逻辑bug而非作者写的bug），但这些问题都不难解决。虽然作者并未提及这些问题，但这些小问题不会影响整体的学习效果。最后，我想强调的是，我们应该多学多用，才能提高自己的编程技能。

